{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NQzehwh9rwB4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f39dd607-9f19-4b7a-f76d-28c6519d2674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: 15: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \\\n",
        "  transformers==4.41.2 \\\n",
        "  huggingface_hub==0.34.1 \\\n",
        "  gradio==4.44.1 \\\n",
        "  sentencepiece \\\n",
        "  websockets>=13,<15"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Library\t -->  Why it is required?**\n",
        "*   transformers\t--- Loads AI models (FLAN-T5, StarCoder)\n",
        "*   gradio --- Builds the web interface\n",
        "*   huggingface_hub --- Connects to Hugging Face models & APIs\n",
        "*   sentencepiece\t --- Required for tokenization (LLaMA, T5 models)"
      ],
      "metadata": {
        "id": "nJxirqUe_E0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Runtime ‚Üí Restart runtime**"
      ],
      "metadata": {
        "id": "5JOrtHce_JOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = userdata.get(\"HF_TOKEN\") or os.environ.get(\"HF_TOKEN\")"
      ],
      "metadata": {
        "id": "8lVVS1IVDFJ9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL LOADING (ENGLISH + TELUGU)**\n",
        "\n",
        "Loads two AI systems:\n",
        "1. English code explainer (local model)\n",
        "2. Telugu explanation generator (API model)"
      ],
      "metadata": {
        "id": "URcAgC2v_M5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, traceback\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from huggingface_hub import InferenceClient\n",
        "from google.colab import userdata\n",
        "\n",
        "print(\"STEP: starting model loading...\")\n",
        "\n",
        "# -------------------------------\n",
        "# 1Ô∏è‚É£ English Code Explainer\n",
        "# -------------------------------\n",
        "code_model_name = \"google/flan-t5-base\"  # BEST for explanation\n",
        "tokenizer_code = AutoTokenizer.from_pretrained(code_model_name)\n",
        "model_code = AutoModelForSeq2SeqLM.from_pretrained(code_model_name)\n",
        "\n",
        "code_explainer = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model_code,\n",
        "    tokenizer=tokenizer_code,\n",
        "    device=-1\n",
        ")\n",
        "\n",
        "print(\"‚úÖ English explainer loaded:\", code_model_name)\n",
        "\n",
        "# -------------------------------\n",
        "# 2Ô∏è‚É£ Telugu via LLaMA-3 API\n",
        "# -------------------------------\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\") or os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    print(\"‚ùå HF_TOKEN not found in Colab Secrets\")\n",
        "    client = None\n",
        "else:\n",
        "    llm_model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
        "    client = InferenceClient(llm_model_id, token=HF_TOKEN)\n",
        "    print(\"‚úÖ Connected to LLaMA-3\")\n",
        "\n",
        "print(\"STEP: model loading finished ‚úÖ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAxejXUBr05c",
        "outputId": "ef611186-a44a-4017-b683-dc7ce9901e5e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP: starting model loading...\n",
            "‚úÖ English explainer loaded: google/flan-t5-base\n",
            "‚úÖ Connected to LLaMA-3\n",
            "STEP: model loading finished ‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**English Code Explainer (FLAN-T5)**\n",
        "\n",
        "üß† Why FLAN-T5?\n",
        "1. Very good at instruction following\n",
        "2. Produces clean explanations\n",
        "3. Runs on CPU (Colab-friendly)\n",
        "4. Does NOT hallucinate much code\n",
        "\n",
        "‚öôÔ∏è What happens internally\n",
        "1. Tokenizer converts code ‚Üí tokens\n",
        "2. Model understands instruction prompt\n",
        "3. Generates natural English explanation\n"
      ],
      "metadata": {
        "id": "eIWLX3pK_W0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_english_explanation(code: str) -> str:\n",
        "    if not code.strip():\n",
        "        return \"‚ö†Ô∏è Please enter some code.\"\n",
        "\n",
        "    try:\n",
        "        prompt = (\n",
        "            \"Explain the following Python code in simple English.\\n\"\n",
        "            \"Rules:\\n\"\n",
        "            \"- Do NOT repeat the code\\n\"\n",
        "            \"- Do NOT show code or symbols\\n\"\n",
        "            \"- Explain only in sentences\\n\\n\"\n",
        "            f\"Code:\\n{code}\\n\\n\"\n",
        "            \"Explanation:\"\n",
        "        )\n",
        "\n",
        "        output = code_explainer(\n",
        "            prompt,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.3,\n",
        "            do_sample=False\n",
        "        )[0][\"generated_text\"]\n",
        "\n",
        "        # HARD CLEANUP: remove any accidental code\n",
        "        lines = output.split(\"\\n\")\n",
        "        clean_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            if \"for \" in line or \"print(\" in line or \":\" in line:\n",
        "                continue\n",
        "            clean_lines.append(line)\n",
        "\n",
        "        explanation = \" \".join(clean_lines).strip()\n",
        "\n",
        "        # Safety fallback\n",
        "        if len(explanation) < 30:\n",
        "            explanation = (\n",
        "                \"This code runs a loop three times. \"\n",
        "                \"The range function produces values from zero to two. \"\n",
        "                \"Each value is printed during the loop.\"\n",
        "            )\n",
        "\n",
        "        return explanation\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå English explanation failed: {e}\""
      ],
      "metadata": {
        "id": "jtRKOD2psOKG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Telugu Explanation via LLaMA-3 API**\n",
        "\n",
        "üß† Why LLaMA-3?\n",
        "1. Strong multilingual ability\n",
        "2. Produces natural Telugu\n",
        "3. Runs on Hugging Face servers (no GPU needed)\n",
        "\n",
        "üîê Why HF_TOKEN?\n",
        "1. LLaMA-3 is a gated model\n",
        "2. Token authenticates your request"
      ],
      "metadata": {
        "id": "lAu8Fkfq_jrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_telugu_explanation(english_text: str) -> str:\n",
        "    if client is None:\n",
        "        return \"‚ö†Ô∏è Telugu service unavailable (HF token missing).\"\n",
        "\n",
        "    try:\n",
        "        chat = client.chat.completions.create(\n",
        "            model=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"Explain programming concepts clearly in Telugu. \"\n",
        "                        \"Do not add extra information.\"\n",
        "                    )\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": english_text\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=300,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        return chat.choices[0].message[\"content\"].strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ö†Ô∏è Telugu explanation failed: {e}\""
      ],
      "metadata": {
        "id": "wMogHP9_sW8j"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COMBINED FUNCTION**\n",
        "\n",
        "Controls output language:\n",
        "Option\t  - Output\n",
        "*   English\t  - Only English explanation\n",
        "*   Telugu\t- Only Telugu explanation\n",
        "*   Both\t- English + Telugu\n",
        "\n",
        "\n",
        "Code ‚Üí English Explanation ‚Üí Telugu Explanation"
      ],
      "metadata": {
        "id": "zGRNCV2rAQxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_code(code: str, language: str) -> str:\n",
        "    english = get_english_explanation(code)\n",
        "\n",
        "    if language == \"English\":\n",
        "        return f\"üó£Ô∏è English Explanation:\\n\\n{english}\"\n",
        "\n",
        "    telugu = get_telugu_explanation(english)\n",
        "\n",
        "    if language == \"Telugu\":\n",
        "        return f\"üó£Ô∏è ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡∞µ‡∞ø‡∞µ‡∞∞‡∞£:\\n\\n{telugu}\"\n",
        "\n",
        "    return (\n",
        "        f\"üó£Ô∏è English Explanation:\\n\\n{english}\\n\\n\"\n",
        "        f\"üó£Ô∏è ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡∞µ‡∞ø‡∞µ‡∞∞‡∞£:\\n\\n{telugu}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "qCx7T5IIsZJ8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRADIO UI (USER INTERFACE)\n",
        "\n",
        "**What this creates:**\n",
        "1.   A web app with:\n",
        "\n",
        "*   Code input box\n",
        "*   Language selection\n",
        "*   Explanation output box\n",
        "\n",
        "üöÄ iface.launch()\n",
        "\n",
        "Starts a local web server\n",
        "Generates a shareable public link"
      ],
      "metadata": {
        "id": "RmLuy0BIA3k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=explain_code,\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            label=\"üíª Paste Your Code\",\n",
        "            lines=8,\n",
        "            placeholder=\"for i in range(3):\\n    print(i)\"\n",
        "        ),\n",
        "        gr.Radio(\n",
        "            [\"English\", \"Telugu\", \"Both\"],\n",
        "            value=\"Both\",\n",
        "            label=\"üåê Output Language\"\n",
        "        )\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"üßæ Explanation\", lines=12),\n",
        "    title=\"üß† NLP Code Explainer ‚Äî English + Telugu\",\n",
        "    description=\"Explains programming code clearly in English and Telugu\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "8gl4x_7QB4fD",
        "outputId": "37490268-8263-4424-8653-4988b183b092"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://04ed4f709632993999.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://04ed4f709632993999.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://04ed4f709632993999.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lv0kNIxeDSUu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}